%!TEX root = ../main.tex
\chapter{Theoretical Foundations of QMC-Based Learning}
\label{chapter5}

\begin{itemize}
    \item Adapting QMC theory to supervised learning
    \item Hardy-Krause variation of the loss function
    \item Generalization error bounds using low-discrepancy samplin\item 
    \item Comparison to MC-based bounds and practical implication\item 
    \item Validity conditions and smoothness assumptions
\end{itemize}

\begin{align*}
    V_{\mathrm{HK}}(f) = V_{\mathrm{HK}}(f; a, b) &= \sum_{u\subset \{1,\dots,s\}} V_{[a^{-u}, b^{-u}]} f(x^{-u}:b^u) \\
    &= \sum_{u\subset \{1,\dots,s\}} \sup_{\mathcal{Y} \in \mathbb{Y}} V_\mathcal{Y} \big( f(x^{-u} : b^u) \big) \\
    &= \sum_{u\subset \{1,\dots,s\}} \sup_{\mathcal{Y} \in \mathbb{Y}} \sum_{y\in\mathcal{Y}} \big| \Delta\big( f(x^{-u} : b^u); y, y_+ \big) \big| \\
    &= \sum_{u\subset \{1,\dots,s\}} \sup_{\mathcal{Y} \in \mathbb{Y}} \sum_{y\in\mathcal{Y}} \big| \sum_{v\subseteq \{1,\dots,s\}} (-1)^{|v|} f((y^v:y_+^{-v})^{-u}:b^{u}) \big| \\
\end{align*}

For the definitions of Variation we follow \cite{owen2005multidimensional} and
start by introducing the mixed power notation:

\begin{definition}[Mixed Component Selection] \ \\
    \label{def:component_merge}
    Let $\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^s$ and let $u \subseteq \{1, \dots, s\}$ be an index set. We define
    \begin{equation*}
        \boldsymbol{c} := \boldsymbol{a}^u : \boldsymbol{b}^{-u}
        \quad \text{with} \quad
        c_j := 
        \begin{cases}
            a_j, & \text{if } j \in u, \\
            b_j, & \text{if } j \notin u.
        \end{cases}
    \end{equation*}
\end{definition}

Hereby the term $-u$ denotes the complement of $u$ in $\{1, \dots, s\}$. Further the alternating sum is defined.

\begin{definition}[Alternating sum] \ \\
    The \emph{alternating sum} on $s$ dimensions with respect to a function on
    the hyperrectangle $[a,b]$ is defined as
    \begin{equation*}
        \Delta(f; a, b) = \sum_{v \subseteq \{1,\dots,s\}} (-1)^{|v|} f(a^v:b^{-v}).
    \end{equation*}
\end{definition}

An essential concept in this context is the notion of a \emph{ladder} which we
define on a certain hypercube $[a,b]$.

\begin{definition}[Ladder] \ \\
    A \emph{ladder} on the hypercube $[a,b]$ is a set of points $\mathcal{Y} =
    \{y_1, \dots, y_k\} \subset [a,b]$ such that for each $i < j$, the point
    $y_i$ is less than or equal to $y_j$ in every coordinate. Formally, this
    means that for all $i < j$ and for all $l \in \{1, \dots, s\}$, we have
    $y_i^l \leq y_j^l$.
\end{definition}

The space of all such ladders on the hypercube is denoted by $\mathbb{Y}$. Next,
the \emph{variation with respect to a ladder} is defined.

\begin{definition}[Variation with respect to a ladder] \ \\
    The \emph{variation} of a function $f$ on the hyperrectangle $[a,b]$
    \emph{with respect to a ladder}
    $\mathcal{Y} = \prod\limits_{j=1}^s \mathcal{Y}^j$ is defined as
    \begin{equation*}
        V_\mathcal{Y}(f) = \sum_{y \in \mathcal{Y}} \big| \Delta(f; y, y_+) \big|,
    \end{equation*}
    where $y_+$ is the point obtained by incrementing each coordinate of $y$
    with $y^j_+$ to be the successor of $y_j$ in $\mathcal{Y}^j$.
\end{definition}

Accordingly the Variation by Vitali is defined as the supremum of the variation
over all ladders $\mathcal{Y} \in \mathbb{Y}$.

\begin{definition}[Vitali Variation] \ \\
    The \emph{Vitali variation} of a function $f$ on the hyperrectangle $[a,b]$ is defined as
    \begin{equation*}
        V(f) = V(f; a, b) = \sup_{\mathcal{Y} \in \mathbb{Y}} V_\mathcal{Y}(f).
    \end{equation*}
    Here $\mathbb{Y}$ denotes the set of all ladders on the hyperrectangle $[a,b]$.
\end{definition}

The Hardy--Krause variation is a specific case of the Vitali variation, where the supremum is taken over all ladders in the unit hypercube $[0,1]^s$.

\begin{definition}[Hardy--Krause Variation] \ \\
    The \emph{Hardy--Krause variation} of a function $f$ on the hyperrectangle
    $[a,b]$ is defined as
    \begin{equation*}
        V_{\mathrm{HK}}(f) = V_{\mathrm{HK}}(f; 0, 1) = \sup_{\mathcal{Y} \in 
        \mathbb{Y}} V_\mathcal{Y}(f).
    \end{equation*}
\end{definition}




% % Chapter Template

% \chapter{Neural Network Training for Stochastic Functions} % Main chapter title

% \label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

% %----------------------------------------------------------------------------------------
% %	SECTION 1
% %----------------------------------------------------------------------------------------

% \section{Modeling Randomness in Physical Simulations}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam ultricies lacinia euismod. Nam tempus risus in dolor rhoncus in interdum enim tincidunt. Donec vel nunc neque. In condimentum ullamcorper quam non consequat. Fusce sagittis tempor feugiat. Fusce magna erat, molestie eu convallis ut, tempus sed arcu. Quisque molestie, ante a tincidunt ullamcorper, sapien enim dignissim lacus, in semper nibh erat lobortis purus. Integer dapibus ligula ac risus convallis pellentesque.

% %----------------------------------------------------------------------------------------
% %	SECTION 2
% %----------------------------------------------------------------------------------------

% \section{Sampling Stochastic Variables with QMC and MC}

% Sed ullamcorper quam eu nisl interdum at interdum enim egestas. Aliquam placerat justo sed lectus lobortis ut porta nisl porttitor. Vestibulum mi dolor, lacinia molestie gravida at, tempus vitae ligula. Donec eget quam sapien, in viverra eros. Donec pellentesque justo a massa fringilla non vestibulum metus vestibulum. Vestibulum in orci quis felis tempor lacinia. Vivamus ornare ultrices facilisis. Ut hendrerit volutpat vulputate. Morbi condimentum venenatis augue, id porta ipsum vulputate in. Curabitur luctus tempus justo. Vestibulum risus lectus, adipiscing nec condimentum quis, condimentum nec nisl. Aliquam dictum sagittis velit sed iaculis. Morbi tristique augue sit amet nulla pulvinar id facilisis ligula mollis. Nam elit libero, tincidunt ut aliquam at, molestie in quam. Aenean rhoncus vehicula hendrerit.

% %----------------------------------------------------------------------------------------
% %	SECTION 3
% %----------------------------------------------------------------------------------------

% \section{Data Generation: Simulating Photon Scatter}

% Sed ullamcorper quam eu nisl interdum at interdum enim egestas. Aliquam placerat justo sed lectus lobortis ut porta nisl porttitor. Vestibulum mi dolor, lacinia molestie gravida at, tempus vitae ligula. Donec eget quam sapien, in viverra eros. Donec pellentesque justo a massa fringilla non vestibulum metus vestibulum. Vestibulum in orci quis felis tempor lacinia. Vivamus ornare ultrices facilisis. Ut hendrerit volutpat vulputate. Morbi condimentum venenatis augue, id porta ipsum vulputate in. Curabitur luctus tempus justo. Vestibulum risus lectus, adipiscing nec condimentum quis, condimentum nec nisl. Aliquam dictum sagittis velit sed iaculis. Morbi tristique augue sit amet nulla pulvinar id facilisis ligula mollis. Nam elit libero, tincidunt ut aliquam at, molestie in quam. Aenean rhoncus vehicula hendrerit.

% %----------------------------------------------------------------------------------------
% %	SECTION 4
% %----------------------------------------------------------------------------------------

% \section{Learning Setup and Experimental Design}

% Sed ullamcorper quam eu nisl interdum at interdum enim egestas. Aliquam placerat justo sed lectus lobortis ut porta nisl porttitor. Vestibulum mi dolor, lacinia molestie gravida at, tempus vitae ligula. Donec eget quam sapien, in viverra eros. Donec pellentesque justo a massa fringilla non vestibulum metus vestibulum. Vestibulum in orci quis felis tempor lacinia. Vivamus ornare ultrices facilisis. Ut hendrerit volutpat vulputate. Morbi condimentum venenatis augue, id porta ipsum vulputate in. Curabitur luctus tempus justo. Vestibulum risus lectus, adipiscing nec condimentum quis, condimentum nec nisl. Aliquam dictum sagittis velit sed iaculis. Morbi tristique augue sit amet nulla pulvinar id facilisis ligula mollis. Nam elit libero, tincidunt ut aliquam at, molestie in quam. Aenean rhoncus vehicula hendrerit.

% %----------------------------------------------------------------------------------------
% %	SECTION 5
% %----------------------------------------------------------------------------------------

% \section{Convergence and Performance Comparison}

% Sed ullamcorper quam eu nisl interdum at interdum enim egestas. Aliquam placerat justo sed lectus lobortis ut porta nisl porttitor. Vestibulum mi dolor, lacinia molestie gravida at, tempus vitae ligula. Donec eget quam sapien, in viverra eros. Donec pellentesque justo a massa fringilla non vestibulum metus vestibulum. Vestibulum in orci quis felis tempor lacinia. Vivamus ornare ultrices facilisis. Ut hendrerit volutpat vulputate. Morbi condimentum venenatis augue, id porta ipsum vulputate in. Curabitur luctus tempus justo. Vestibulum risus lectus, adipiscing nec condimentum quis, condimentum nec nisl. Aliquam dictum sagittis velit sed iaculis. Morbi tristique augue sit amet nulla pulvinar id facilisis ligula mollis. Nam elit libero, tincidunt ut aliquam at, molestie in quam. Aenean rhoncus vehicula hendrerit.