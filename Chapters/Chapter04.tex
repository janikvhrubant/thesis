%!TEX root = ../main.tex
\part{Quasi-Monte Carlo Sampling for Neural Network Training}
\label{part2}

\chapter{Motivation and Problem Formulation}
\label{chapter4}
The second part of this thesis investigates the application of \acfp{qmc}
sampling techniques to enhance the training of \acfp{dnn} in \emph{many-query
problems}, computational tasks where the same expensive model must be evaluated
repeatedly across a large number of different input parameters.
Chapter~\ref{chapter4} introduces the conceptual foundations of \acp{dnn}.
Chapter~\ref{chapter5} presents the theoretical framework for evaluating the
quality of neural network training, including the notions of generalization and
training error, along with corresponding bounds. Building on this,
Chapter~\ref{chapter6} applies these theoretical insights to the training of
neural networks aimed at approximating three representative problems: a
multidimensional sum-of-sines function, the trajectory of a projectile from
classical physics and the pricing of a European call option in computational
finance.

\textcolor{red}{\textbf{TODO: Last Chapter of Part 2 to be added here.}}

\qquad

\acp{dnn} are machine learning models, designed to approximate complex nonlinear
relationships in data. Rather than producing exact solutions in a mathematical
sense, they learn from examples and generate predictions that often serve as
good approximations. This makes them particularly well suited for approximating
parameterized functions in scientific and engineering contexts, especially when
the underlying problems are analytically unsolvable, computationally expensive,
or only implicitly defined.

In the remainder of this chapter, we provide a mathematical formulation of
\acp{dnn}, along with a precise definition of the function approximation tasks
they are designed to address.

% ------------------------------------------------------------------------------
\section{Function Approximation in Many-Query Problems}
\label{sec:problem-setting}
% ------------------------------------------------------------------------------

% -------------------------------
\subsection{Problem Setting}
\label{subsec:problem-setting}
% -------------------------------

Building on the previously introduced notion of many-query problems, we now formalize the underlying function approximation task. The objective is to learn a function \( f \colon \mathcal{X} \rightarrow \mathcal{Y} \) that maps high-dimensional input parameters to corresponding outputs of interest. Typically, \( \mathcal{X} \subseteq \mathbb{R}^d \) is compact and taken to be the unit hypercube, \( \mathcal{X} = [0,1]^d \), while \( \mathcal{Y} \) is a Banach space such as \( \mathbb{R} \) or \( L^p(\Omega) \) for some domain \( \Omega \subseteq \mathbb{R}^n \).

The surrogate model \( \hat{f}_\theta \) is typically chosen from a class of deep neural networks and is trained on a finite dataset \( \{ (x_i, f(x_i)) \}_{i=1}^N \), where \( x_i \in [0,1]^d \). This setting is particularly relevant when evaluating \( f \) is expensive (e.g., solving a PDE or performing a simulation), but sampling inputs is cheap and parallelizable.

\begin{remark}
The function \( f \) may not admit a closed-form expression or may be computationally infeasible to evaluate for every input configuration. Therefore, a learned approximation \( \hat{f}_\theta \), trained on a representative set of inputs, offers a practical alternative for many-query tasks such as uncertainty quantification, optimization, or inverse problems.
\end{remark}

% -------------------------------
\subsection{Target Function Class}
\label{subsec:function-class}
% -------------------------------

\begin{definition}[Target Function]
Let \( d \in \mathbb{N} \), and let \( f \colon [0,1]^d \rightarrow \mathcal{Y} \) be a measurable function, where \( \mathcal{Y} \) is a real Banach space. The goal is to construct a parametric surrogate model \( \hat{f}_\theta \) that approximates \( f \) in a suitable norm, i.e., \( \| f - \hat{f}_\theta \| \ll 1 \).
\end{definition}

To ensure meaningful approximation guarantees, assumptions on the regularity of \( f \) are often imposed. Common choices include:
\begin{itemize}
  \item \textbf{Lipschitz continuity}, ensuring bounded gradients,
  \item Membership in \textbf{Sobolev spaces} \( W^{k,p} \), which incorporate smoothness and integrability,
  \item \textbf{Bounded variation}, relevant for functions with discontinuities.
\end{itemize}

The specific regularity assumption affects both the approximation power of the neural network and the theoretical error bounds discussed in later chapters.
