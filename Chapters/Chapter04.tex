%!TEX root = ../main.tex
\part{Quasi-Monte Carlo Sampling for Neural Network Training}
\label{part2}

\chapter{Motivation and Problem Formulation}
\label{chapter4}

The second part of this thesis investigates the application of \acfp{qmc}
sampling techniques to enhance the training of \acfp{dnn} in \emph{many-query
problems}—computational tasks in which a parameterized map must be evaluated for
a large number of inputs, each requiring costly simulations or numerical
solutions of PDEs. This situation arises frequently in scientific computing,
e.g., in uncertainty quantification, Bayesian inversion, optimal control, and
model calibration.

Such problems motivate the use of surrogate models $\hat{f}_\theta$, typically
based on deep neural networks, which are trained offline on a set of inputs and
then used for fast evaluation online. However, using random sampling for
generating training data yields to star discrepancy $\mathcal{O}(N^{-1/2})$ as
introduced in Parth \ref{part1}.

To overcome this limitation, this thesis explores the use of low-discrepancy
(QMC) sequences as training data. Such sequences fill the input domain more
uniformly and, under mild regularity assumptions, can significantly reduce the
generalization error due to improved equidistribution properties.

\section{Many-Query Problems in Scientific Computing}
Many-query problems refer to scenarios in which a computationally expensive
model $f(x)$ must be evaluated for many different parameter values $x \in
\mathcal{X} \subset \mathbb{R}^s$. Typical applications include:
\begin{itemize}
  \item \textbf{Uncertainty quantification (UQ)}: Compute statistics of $f(x)$
  for uncertain $x$.
  \item \textbf{Optimal design/control}: Evaluate objective functions $f(x)$ for
  many designs $x$.
  \item \textbf{Inverse problems}: Repeatedly evaluate $f(x)$ during parameter
  inference.
\end{itemize}

The sheer cost of evaluating $f(x)$ (e.g., via numerical PDE solvers) motivates
the use of surrogate models. Once trained, a surrogate $\hat{f}_\theta$ can
provide rapid predictions for unseen inputs at a fraction of the computational
cost.

\section{The Role of Function Approximation}
We formalize the problem as learning a map $f \colon [0,1]^s \to \mathbb{R}^m$
from data. The assumption of a unit hypercube domain reflects common practice in
QMC theory and is justified via homeomorphic mappings from more general compact
input spaces.

The surrogate model $\hat{f}_\theta$ is chosen from a class of deep neural
networks and trained on data $\{(x_i, f(x_i))\}_{i=1}^N$. The aim is to
approximate $f$ with high accuracy while minimizing the number of required
evaluations.

\section{Limitations of Random Sampling in Neural Network Training}
Training data is typically generated by sampling $x_i \sim \mu$ independent and identicaly distributed (i.i.d.) from a
given probability distribution on $[0,1]^s$. In this case, theoretical results
in statistical learning theory yield generalization bounds of the form
\begin{equation*}
\mathbb{E}[E_G] \leq E_T + \mathcal{O}\left(\frac{1}{\sqrt{N}}\right),
\end{equation*}
where $E_T$ is the empirical training error. However, as argued
in~\cite{mishra2021enhancing}, this rate is unsatisfactory in many-query
problems because:
\begin{itemize}
  \item A small generalization error requires a large $N$, which is costly due
  to expensive evaluations of $f(x)$.
  \item The constant in the bound depends on the variance and correlations in
  the training process, which are hard to control.
\end{itemize}

\section{QMC-Based Surrogates: A Promising Alternative}
Low-discrepancy sequences (Sobol', Halton, etc.) provide a way to sample the
domain $[0,1]^s$ more uniformly than random sampling. For sufficiently smooth
functions $f$, Quasi-Monte Carlo theory (e.g., Koksma--Hlawka inequality) gives
error bounds of the form:
\begin{equation*}
|E_G - E_T| \leq V_{\mathrm{HK}}(|f - \hat{f}_\theta|) \cdot D^*_N \leq C \frac{(\log N)^s}{N},
\end{equation*}
where $V_{\mathrm{HK}}$ denotes the Hardy–Krause variation and $D^*_N$ is the
star-discrepancy of the training points.

This suggests that QMC sampling can lead to significantly smaller generalization
gaps—especially when $f$ has bounded variation and the activation functions used
in $\hat{f}_\theta$ are smooth.

\section{Overview of Part~\ref{part2}} The remainder of this part is structured
as follows:
\begin{itemize}
  \item \textbf{Chapter~\ref{chapter5}} introduces the mathematical foundations
  of deep neural networks and their use in function approximation.
  \item \textbf{Chapter~\ref{chapter6}} discusses the concepts of training and
  generalization error, including classical and QMC-based bounds.
  \item \textbf{Chapter~\ref{chapter7}} presents the theoretical analysis of
  QMC-based training, including discrepancy, variation, and convergence
  guarantees.
  \item \textbf{Chapter~\ref{chapter8}} provides an empirical evaluation
  comparing QMC and Monte Carlo training strategies across different benchmark
  tasks.
\end{itemize}

The ultimate goal of this part is to demonstrate that quasi-random sampling
provides a viable and theoretically grounded strategy to reduce the cost and
improve the accuracy of deep neural network training in scientific computing.

