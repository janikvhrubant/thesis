%!TEX root = ../main.tex
\part{Quasi-Monte Carlo Sampling for Neural Network Training}
\label{part2}

\chapter{Motivation and Problem Formulation}
\label{chapter4}

The second part of this thesis investigates the application of \acfp{qmc}
sampling techniques to enhance the training of \acfp{dnn} in \emph{many-query
problems} -- computational tasks in which a parameterized map must be evaluated for
a large number of inputs, each requiring costly simulations or numerical
solutions of PDEs. This situation arises frequently in scientific computing,
e.g. in uncertainty quantification, Bayesian inversion, optimal control and
model calibration.

Such problems motivate the use of surrogate models $\hat{f}_\theta$, typically
based on deep neural networks, which are trained offline on a set of inputs and
then used for fast evaluation online. However, using random sampling for
generating training data yields to star discrepancy $\mathcal{O}(N^{-1/2})$ as
introduced in Part~\ref{part1}.

To overcome this limitation, this thesis explores the use of low-discrepancy sequences as training data which fill the input domain more
uniformly. Under mild regularity assumptions, they can significantly reduce the
generalization error due to improved equidistribution properties.

\section{Many-Query Problems in Scientific Computing}
Many-query problems refer to scenarios in which a computationally expensive
model $f(x)$ must be evaluated for many different parameter values $x \in
\mathcal{X} \subset \mathbb{R}^s$. Typical applications include:
\begin{itemize}
  \item \textbf{Uncertainty quantification (UQ)}: Compute statistics of $f(x)$
  for uncertain $x$.
  \item \textbf{Optimal design/control}: Evaluate objective functions $f(x)$ for
  many designs $x$.
  \item \textbf{Inverse problems}: Repeatedly evaluate $f(x)$ during parameter
  inference.
\end{itemize}

The sheer cost of evaluating $f(x)$ (e.g., via numerical PDE solvers) motivates
the use of surrogate models. Once trained, a surrogate $\hat{f}_\theta$ can
provide rapid predictions for unseen inputs at a fraction of the computational
cost.

\section{The Role of Function Approximation}
We formalize the problem as learning a map $f \colon [0,1]^s \to \mathbb{R}^m$
from data. The assumption of a unit hypercube domain reflects common practice in
\ac{qmc} theory and is justified via homeomorphic mappings from more general
compact and simply connected input spaces.

The surrogate model $\hat{f}_\theta$ can be chosen from the class of deep neural
networks and trained on data $\{(x_i, f(x_i))\}_{i=1}^N$. The aim is to
approximate $f$ with high accuracy while minimizing the number of required
evaluations.

\aclp{dnn} are known to be universal approximators. Under mild conditions on the
later introduced \emph{activation functions} they proved to be able to
approximate any continuous function on compact sets arbitrarily well
\cite{cybenko1989approximation}.

\section{Limitations of Random Sampling in Neural Network Training}
Typically, training data is generated using \acl{mc} sampling, where points $x_i$ are drawn independently and identically distributed (i.i.d.) according to a probability measure $\mu$ on $[0,1]^s$. Under this approach, statistical learning theory provides an estimate for the expected generalization error $\mathbb{E}[E_G]$ as follows:
\begin{equation*}
\mathbb{E}[E_G] \leq E_T + \mathcal{O}\left(\frac{1}{\sqrt{N}}\right),
\end{equation*}
where $E_T$ is the empirical training error. However, as argued
in~\cite{mishra2021enhancing}, this rate is unsatisfactory in many-query
problems because:
\begin{itemize}
  \item A small generalization error requires a large $N$, which is costly due
  to expensive evaluations of $f(x)$.
  \item The constant in the bound depends on the variance and correlations in
  the training process, which are hard to control.
\end{itemize}

\section{QMC-Based Surrogates: A Promising Alternative}
Low-discrepancy sequences (Sobol', Halton, etc.) provide a way to sample the
domain $[0,1]^s$ more uniformly than random sampling. In \acl{qmc} theory, the Koksma-Hlawka inequality (Theorem~\ref{thm:koksma-hlawka}) gives an error bound of the form
\begin{equation*}
|E_G - E_T| \leq V_{\mathrm{HK}}(|f - \hat{f}_\theta|) \cdot D^*_N \leq  V_{\mathrm{HK}}(|f - \hat{f}_\theta|) \cdot C \frac{(\log N)^s}{N},
\end{equation*}
where $V_{\mathrm{HK}}$ denotes the Hardy--Krause variation and $D^*_N$ is the
star-discrepancy of the training points.

This suggests that \ac{qmc} sampling can lead to significantly smaller generalization
gaps -- especially when $f$ has bounded variation and the activation functions used
in $\hat{f}_\theta$ are smooth.

\section{Overview of Part~\ref{part2}} The remainder of this part is structured
as follows:
\begin{itemize}
  \item \textbf{Chapter~\ref{chapter5}} introduces the mathematical foundations
  of deep neural networks and their use in function approximation.
  \item \textbf{Chapter~\ref{chapter6}} discusses the concepts of training and
  generalization error, including classical and QMC-based bounds.
  \item \textbf{Chapter~\ref{chapter7}} presents the theoretical analysis of
  QMC-based training, including discrepancy, variation, and convergence
  guarantees.
  \item \textbf{Chapter~\ref{chapter8}} provides an empirical evaluation
  comparing QMC and Monte Carlo training strategies across different benchmark
  tasks.
\end{itemize}

The ultimate goal of this part is to demonstrate that quasi-random sampling
provides a viable and theoretically grounded strategy to reduce the cost and
improve the accuracy of deep neural network training in scientific computing.

